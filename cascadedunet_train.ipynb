{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce6bb0a6",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce3c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[pillow, tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e22cd78",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eacb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.config import print_config\n",
    "from monai.data import (CacheDataset, DataLoader, decollate_batch,\n",
    "                        load_decathlon_datalist)\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (AsDiscrete, Compose, EnsureChannelFirstd,\n",
    "                              LoadImaged, NormalizeIntensityd, Orientationd,\n",
    "                              RandAdjustContrastd, RandBiasFieldd,\n",
    "                              RandCropByPosNegLabeld, RandFlipd,\n",
    "                              RandGibbsNoised, RandHistogramShiftd,\n",
    "                              RandKSpaceSpikeNoised, RandRotate90d, Spacingd,\n",
    "                              SpatialPadd, ToTensord)\n",
    "from monai.utils import set_determinism\n",
    "from tqdm import tqdm\n",
    "\n",
    "from swin_unetr.cascaded_unet import CascadedUNet\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc2c357",
   "metadata": {},
   "source": [
    "##### Note:\n",
    "Define training as decathlon dataset with training/validation split. Also set directories, and detect number of labels that the network must predict.\n",
    "\n",
    "The feature_nets list contains the path to pre-trained feature nets, and the number of output channels predicted by those networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7cffc24",
   "metadata": {},
   "source": [
    "##### Define file paths & output directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e2c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = os.path.normpath(\"D:/lloyd/datasets/CC/charm_corrected6_t1.json\")\n",
    "data_dir = os.path.normpath(\"D:/lloyd/datasets/CC\")\n",
    "logdir = os.path.normpath(\"D:/lloyd/datasets/CC/charm_corrected6_cascade_unet\")\n",
    "labels = json.loads(Path(json_path).read_text())[\"labels\"]\n",
    "labels = {int(k): v for k, v in labels.items()}\n",
    "num_classes = max(labels.keys()) + 1\n",
    "feature_nets = [\n",
    "    (\n",
    "        4,\n",
    "        \"D:/lloyd/datasets/CC/WM-GM-CSF_train/training/3/epoch=488-val_loss=0.06-val_dice=0.9217.pth\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "if os.path.exists(logdir) is False:\n",
    "    os.mkdir(logdir)\n",
    "\n",
    "set_determinism(seed=960)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ce7a7aa",
   "metadata": {},
   "source": [
    "##### Defined flag to utilize pre-trained weights and path to pre-trained weights. If flag is set to 'False', random initialization will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0dadec",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained = False\n",
    "pretrained_path = os.path.normpath(os.path.join(logdir, \"best_metric_model.pth\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d9e4650",
   "metadata": {},
   "source": [
    "##### MONAI Transforms for training and validation, training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyper-params\n",
    "lr = 4e-4\n",
    "max_iterations = 30000\n",
    "eval_num = 100\n",
    "\n",
    "# Transforms\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"], reader=\"ITKReader\"),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=False, channel_wise=True),\n",
    "        # CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        SpatialPadd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_size=(96, 96, 96),\n",
    "        ),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            label_key=\"label\",\n",
    "            spatial_size=(96, 96, 96),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=4,\n",
    "            image_key=\"image\",\n",
    "            image_threshold=0,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[0],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[1],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[2],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandRotate90d(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            prob=0.10,\n",
    "            max_k=3,\n",
    "        ),\n",
    "        RandAdjustContrastd(keys=\"image\", prob=0.2, gamma=(0.5, 4.5)),\n",
    "        RandHistogramShiftd(keys=\"image\", prob=0.2, num_control_points=10),\n",
    "        RandBiasFieldd(keys=\"image\", prob=0.2),\n",
    "        # RandRicianNoised(keys=\"image\", prob=0.1),\n",
    "        RandGibbsNoised(keys=\"image\", prob=0.2, alpha=(0.0, 1.0)),\n",
    "        RandKSpaceSpikeNoised(keys=\"image\", prob=0.2),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"], reader=\"ITKReader\"),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=False, channel_wise=True),\n",
    "        # CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8eefaf7e",
   "metadata": {},
   "source": [
    "##### Load data list and create dataloaders for training\n",
    "\n",
    "Since there is a mismatch between the spacing and the affine matrix in the BTCV dataset, users will see warnings \"pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\".\n",
    "This is expected, and not affecting the results in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf4164",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = load_decathlon_datalist(\n",
    "    base_dir=data_dir,\n",
    "    data_list_file_path=json_path,\n",
    "    is_segmentation=True,\n",
    "    data_list_key=\"training\",\n",
    ")\n",
    "\n",
    "val_files = load_decathlon_datalist(\n",
    "    base_dir=data_dir,\n",
    "    data_list_file_path=json_path,\n",
    "    is_segmentation=True,\n",
    "    data_list_key=\"validation\",\n",
    ")\n",
    "\n",
    "\n",
    "train_ds = CacheDataset(\n",
    "    data=datalist,\n",
    "    transform=train_transforms,\n",
    "    cache_num=24,\n",
    "    cache_rate=1.0,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "val_ds = CacheDataset(\n",
    "    data=val_files, transform=val_transforms, cache_num=6, cache_rate=1.0, num_workers=4\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=2, shuffle=True, num_workers=2, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=1, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "# Sanity check for shapes from data loaders\n",
    "case_num = 0\n",
    "img = val_ds[case_num][\"image\"]\n",
    "label = val_ds[case_num][\"label\"]\n",
    "img_shape = img.shape\n",
    "label_shape = label.shape\n",
    "print(f\"image shape: {img_shape}, label shape: {label_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2076197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CascadedUNet(\n",
    "    in_channels=1,\n",
    "    feature_channel_list=[p[0] for p in feature_nets],\n",
    "    out_channels=num_classes,\n",
    ")\n",
    "\n",
    "if use_pretrained is True:\n",
    "    print(f\"Loading Weights from the Path {pretrained_path}\")\n",
    "    model_state_dict = torch.load(pretrained_path)\n",
    "    model.load_state_dict(model_state_dict, strict=True)\n",
    "else:\n",
    "    for idx, num_path in enumerate(feature_nets):\n",
    "        model_state_dict = torch.load(num_path[1])\n",
    "        model.feature_nets[idx].load_state_dict(model_state_dict, strict=True)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "post_label = AsDiscrete(to_onehot=num_classes)\n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=num_classes)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "global_step = 0\n",
    "dice_val_best = 0.0\n",
    "global_step_best = 0\n",
    "epoch_loss_values = []\n",
    "metric_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch_iterator_val):\n",
    "    model.eval()\n",
    "    dice_vals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _step, batch in enumerate(epoch_iterator_val):\n",
    "            val_inputs, val_labels = (\n",
    "                batch[\"image\"].to(device),\n",
    "                batch[\"label\"].to(device),\n",
    "            )\n",
    "            val_outputs = sliding_window_inference(val_inputs, (96, 96, 96), 4, model)\n",
    "            val_labels_list = decollate_batch(val_labels)\n",
    "            val_labels_convert = [\n",
    "                post_label(val_label_tensor) for val_label_tensor in val_labels_list\n",
    "            ]\n",
    "            val_outputs_list = decollate_batch(val_outputs)\n",
    "            val_output_convert = [\n",
    "                post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list\n",
    "            ]\n",
    "            dice_metric(y_pred=val_output_convert, y=val_labels_convert)\n",
    "            dice = dice_metric.aggregate().item()\n",
    "            dice_vals.append(dice)\n",
    "            epoch_iterator_val.set_description(\n",
    "                \"Validate (%d / %d Steps) (dice=%2.5f)\" % (global_step, 10.0, dice)\n",
    "            )\n",
    "\n",
    "        dice_metric.reset()\n",
    "\n",
    "    mean_dice_val = np.mean(dice_vals)\n",
    "    return mean_dice_val\n",
    "\n",
    "\n",
    "def train(global_step, train_loader, dice_val_best, global_step_best):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    epoch_iterator = tqdm(\n",
    "        train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True\n",
    "    )\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        step += 1\n",
    "        x, y = (batch[\"image\"].to(device), batch[\"label\"].to(device))\n",
    "        logit_map = model(x)\n",
    "        loss = loss_function(logit_map, y)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_iterator.set_description(\n",
    "            \"Training (%d / %d Steps) (loss=%2.5f)\"\n",
    "            % (global_step, max_iterations, loss)\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            global_step % eval_num == 0 and global_step != 0\n",
    "        ) or global_step == max_iterations:\n",
    "            epoch_iterator_val = tqdm(\n",
    "                val_loader, desc=\"Validate (X / X Steps) (dice=X.X)\", dynamic_ncols=True\n",
    "            )\n",
    "            dice_val = validation(epoch_iterator_val)\n",
    "\n",
    "            epoch_loss /= step\n",
    "            epoch_loss_values.append(epoch_loss)\n",
    "            metric_values.append(dice_val)\n",
    "            if dice_val > dice_val_best:\n",
    "                dice_val_best = dice_val\n",
    "                global_step_best = global_step\n",
    "                torch.save(\n",
    "                    model.state_dict(), os.path.join(logdir, \"best_metric_model.pth\")\n",
    "                )\n",
    "                print(\n",
    "                    \"Model Was Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(\n",
    "                        dice_val_best, dice_val\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"Model Was Not Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(\n",
    "                        dice_val_best, dice_val\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            plt.figure(1, (12, 6))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title(\"Iteration Average Loss\")\n",
    "            x = [eval_num * (i + 1) for i in range(len(epoch_loss_values))]\n",
    "            y = epoch_loss_values\n",
    "            plt.xlabel(\"Iteration\")\n",
    "            plt.plot(x, y)\n",
    "            plt.grid()\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title(\"Val Mean Dice\")\n",
    "            x = [eval_num * (i + 1) for i in range(len(metric_values))]\n",
    "            y = metric_values\n",
    "            plt.xlabel(\"Iteration\")\n",
    "            plt.plot(x, y)\n",
    "            plt.grid()\n",
    "            plt.savefig(os.path.join(logdir, \"btcv_finetune_quick_update.png\"))\n",
    "            plt.clf()\n",
    "            plt.close(1)\n",
    "\n",
    "        global_step += 1\n",
    "    return global_step, dice_val_best, global_step_best\n",
    "\n",
    "\n",
    "while global_step < max_iterations:\n",
    "    global_step, dice_val_best, global_step_best = train(\n",
    "        global_step, train_loader, dice_val_best, global_step_best\n",
    "    )\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(logdir, \"final_iteration_model.pth\"))\n",
    "model.load_state_dict(torch.load(os.path.join(logdir, \"best_metric_model.pth\")))\n",
    "\n",
    "print(\n",
    "    f\"train completed, best_metric: {dice_val_best:.4f} \"\n",
    "    f\"at iteration: {global_step_best}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21407efe",
   "metadata": {},
   "source": [
    "##### Visualize the training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae434c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Iteration Average Loss\")\n",
    "x = [eval_num * (i + 1) for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [eval_num * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
